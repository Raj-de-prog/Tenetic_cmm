{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5123279d-ab06-40c0-9ffa-0aa3f9b84d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import split, size,max\n",
    "\n",
    "df = spark.read.format('text').option('compression', 'gzip').load('s3a://consumer-orbit-us-east-1-holding/October2024_TCI/*.gz')\n",
    "df_split = df.withColumn(\"split_col\", split(df[\"value\"], \"\\t\"))\n",
    "\n",
    "df_split_size=df_split.select('split_col',size('split_col').alias(\"No_of_Columns\"))\n",
    "\n",
    "MaxNumElements=df_split_size.agg({'No_of_Columns': 'max'}).collect()[0][0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a68300-d3fb-4bf2-b5f6-0ed134c20234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_split_size.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d748ff3-171b-4c54-ab71-d6dfb0039a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def arraySplitIntoCols (df,No_of_Columns):\n",
    "    for i in range(No_of_Columns):\n",
    "        df = df.withColumn(f\"col{i}\", df.split_col[i])\n",
    "    \n",
    "    return df    \n",
    "\n",
    "df_final=arraySplitIntoCols(df_split_size,MaxNumElements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d67e2d-5c28-4e64-8e01-25e37a006476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.drop('split_col').drop('No_of_Columns').display()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389fd426-4b54-4cad-8a70-5c57f83423dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "df = pandas.read_excel('s3://consumer-orbit-us-east-1-holding/Total_Consumer_Insights_Counts (9) - Copy.xlsx', sheet_name='sheet1')\n",
    "sdf = spark.createDataFrame(df)\n",
    "\n",
    "\n",
    "##pd.read_excel('s3://consumer-orbit-us-east-1-holding/Total_Consumer_Insights_Counts (9) - Copy.xlsx', index_col=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f909b21c-f522-42e6-87d0-085a0962fdb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('s3://consumer-orbit-us-east-1-holding/Total_Consumer_Insights_Counts (9) - Copy.xlsx')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a526911-bd1a-4cfc-b788-36805e0089f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Insight_Plus = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('s3://consumer-orbit-us-east-1-holding/Total_Consumer_Insights_Plus_Counts (5) (1).xlsx')\n",
    "\n",
    "display(df_Insight_Plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ee77f5-0c13-4d89-9fa2-4a0ed0f9e5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "##%pip install s3fs\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read Excel file from S3\n",
    "df = pd.read_excel('s3://consumer-orbit-us-east-1-holding/Total_Consumer_Insights_Counts (9) - Copy.xlsx', sheet_name='sheet1')\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "sdf = spark.createDataFrame(df)\n",
    "\n",
    "# Display the Spark DataFrame\n",
    "display(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26301fff-9a0f-4f2f-8b78-80658f63522a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "##%pip install s3fs\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read Excel file from S3\n",
    "df = pd.read_excel('s3://consumer-orbit-us-east-1-holding/Total_Consumer_Insights_Counts (9) - Copy.xlsx', sheet_name='sheet1')\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "sdf = spark.createDataFrame(df)\n",
    "\n",
    "# Display the Spark DataFrame\n",
    "display(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96338812-0a99-4558-823e-0a4ec935d205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b66fac-ddf2-4d56-84a7-b7f217a051cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import boto3\n",
    "\n",
    "# Replace with your actual AWS credentials\n",
    "ACCESS_KEY = 'ASIA25EAKRJK2JMDQMJ7'\n",
    "SECRET_KEY = 'w9hN7xV0GUNjiW7+Qf/Yq4Te5i4GdmQNWcwJUrie'\n",
    "AWS_BUCKET_NAME = 'consumer-orbit-us-east-1-holding'\n",
    "\n",
    "# Create a session using your credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# List the contents of the bucket\n",
    "response = s3.list_objects_v2(Bucket=AWS_BUCKET_NAME)\n",
    "\n",
    "# Display the contents\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Match Layout",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
